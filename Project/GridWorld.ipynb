{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import IPython.display\n",
    "from time import sleep\n",
    "import pickle\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from keras.engine import data_adapter\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987413c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(env=None, agent=None, i=None, reward=None, clear=False):\n",
    "    if clear:\n",
    "        IPython.display.clear_output(True)\n",
    "    if i is not None:\n",
    "        print(i)\n",
    "    if reward is not None:\n",
    "        print(reward)\n",
    "    if env is not None:\n",
    "        env.render()\n",
    "    if agent is not None:\n",
    "        print('cur_eps', agent.cur_eps)\n",
    "        agent.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('FrozenLake-v1',  map_name='4x4', is_slippery=False)#desc=['SH', 'FG'],\n",
    "env = gym.make('CartPole-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd737d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.reset()\n",
    "    done = False\n",
    "    actions = [2, 2, 1, 1, 1, 2]\n",
    "    for action in actions:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print_stats(env)\n",
    "        # sleep(1)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7cd87-d434-41ab-b619-2f9a665bbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_gen(set_eps):\n",
    "        decay = 1.00001\n",
    "        epsilon = set_eps\n",
    "        while True:\n",
    "            yield epsilon\n",
    "            epsilon /= decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c35ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, eps_gen, init_weights=0, size=4, lr=0.45, discount=0.95):\n",
    "        self.moves = 4\n",
    "        self.q_table = np.ones((size*size, self.moves)) * init_weights\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.eps_gen = eps_gen\n",
    "        \n",
    "    def set_eps_gen(self, eps_gen):\n",
    "        self.eps_gen = eps_gen\n",
    "        \n",
    "    def pick_move(self, observation, greedy=False):\n",
    "        self.lastpos = observation\n",
    "        self.cur_eps = next(self.eps_gen)\n",
    "        r = np.random.random()\n",
    "        if r < self.cur_eps and not greedy:\n",
    "            self.last_action = np.random.choice(self.moves) # explore\n",
    "        else:\n",
    "            self.last_action = np.argmax(self.q_table[observation]) # exploit\n",
    "        return self.last_action\n",
    "        \n",
    "    def update_table(self, observation, reward):\n",
    "        calc = reward\n",
    "        calc += self.discount * np.max(self.q_table[observation])\n",
    "        self.q_table[self.lastpos][self.last_action] = (1-self.lr) * self.q_table[self.lastpos][self.last_action] + self.lr * calc\n",
    "#         self.print_weights()\n",
    "    def print_weights(self):\n",
    "        print(self.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b10845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute this cell if you don't want to use Q-Learning\n",
    "\n",
    "agent = QLearningAgent(eps_gen(1), size=2)\n",
    "for i in range(1000):\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = agent.pick_move(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print_stats(env, agent, i)\n",
    "        agent.update_table(obs, reward)\n",
    "        if done:\n",
    "            eps = 1\n",
    "            break\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7d429-73d7-480e-8066-c229f5df96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearningAgent:\n",
    "    def __init__(self, eps_gen, env, hidden=None, size=4, lr=0.7, discount=0.95, set_eps=1, replay_memory_capacity=500):\n",
    "        self.eps_gen = eps_gen\n",
    "        self.cur_eps = None\n",
    "        self.moves = env.action_space.n\n",
    "        print('moves', self.moves)\n",
    "        self.size = size\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.cart_pole = not isinstance(env.env, gym.envs.toy_text.FrozenLakeEnv)\n",
    "        \n",
    "        if self.cart_pole:\n",
    "            self.model = K.Sequential([\n",
    "                K.Input(shape=env.observation_space.shape)\n",
    "            ])\n",
    "        else:\n",
    "            self.model = K.Sequential([\n",
    "                K.Input(shape=(2,))])\n",
    "        \n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = []\n",
    "            print('`hidden` was not provided, using 0 hidden layers')\n",
    "        for size in hidden:\n",
    "            print(size)\n",
    "            self.model.add(K.layers.Dense(size, activation=\"relu\"))\n",
    "        self.model.add(K.layers.Dense(self.moves))\n",
    "        \n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='huber')\n",
    "        \n",
    "        config = self.model.get_config()\n",
    "        self.target_model = K.Sequential.from_config(config)\n",
    "        self.save_target_model()\n",
    "        \n",
    "        self.buf_shape = (replay_memory_capacity, )\n",
    "        self.s_buf = np.zeros(self.buf_shape + env.observation_space.shape)\n",
    "        self.a_buf = np.zeros(self.buf_shape)\n",
    "        self.r_buf = np.zeros(self.buf_shape)\n",
    "        self.t_buf = np.zeros(self.buf_shape)\n",
    "        self.s_prime_buf = np.zeros(self.buf_shape + env.observation_space.shape)\n",
    "        self.buf_index = 0\n",
    "        \n",
    "    def set_eps_gen(self, eps_gen):\n",
    "        self.eps_gen = eps_gen\n",
    "    \n",
    "    def transform_observation(self, observation):\n",
    "        return np.array([observation // self.size, observation % self.size])\n",
    "    \n",
    "    def pick_move(self, observation, random=False, greedy=False, verbose=False):\n",
    "        if random:\n",
    "            return np.random.choice(self.moves)\n",
    "        if greedy:\n",
    "            if self.cart_pole:\n",
    "                observation = np.expand_dims(observation, 0)\n",
    "            else:\n",
    "                observation = np.expand_dims(self.transform_observation(observation), 0)\n",
    "            if verbose:\n",
    "                print(observation)\n",
    "            q_values = self.model.predict(observation)\n",
    "            action = np.argmax(q_values) # exploit\n",
    "            if verbose:\n",
    "                print(q_values)\n",
    "            return action\n",
    "        \n",
    "        # eps-greedy\n",
    "        self.cur_eps = next(self.eps_gen)\n",
    "        r = np.random.random()\n",
    "        if r < self.cur_eps:\n",
    "            action = np.random.choice(self.moves) # explore\n",
    "        else:\n",
    "            if self.cart_pole:\n",
    "                observation = np.expand_dims(observation, 0)\n",
    "            else:\n",
    "                observation = np.expand_dims(self.transform_observation(observation), 0)\n",
    "            \n",
    "            q_values = self.model.predict(observation)\n",
    "            if verbose:\n",
    "                print(q_values)\n",
    "            action = np.argmax(q_values) # exploit\n",
    "            return action\n",
    "        return action\n",
    "    \n",
    "    def save_sarts(self, sarts): # state, action, reward, terminal, state prime\n",
    "        self.s_buf[self.buf_index] = sarts[0]\n",
    "        self.a_buf[self.buf_index] = sarts[1]\n",
    "        self.r_buf[self.buf_index] = sarts[2]\n",
    "        self.t_buf[self.buf_index] = sarts[3]\n",
    "        self.s_prime_buf[self.buf_index] = sarts[4]\n",
    "        self.buf_index = (self.buf_index + 1) % self.buf_shape[0]\n",
    "    \n",
    "    def save_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def update_model(self, size=100, verbose=False):\n",
    "        choice = np.random.choice(self.s_buf.shape[0], size=size)\n",
    "        \n",
    "        if self.cart_pole:\n",
    "            obs = self.s_buf[choice]\n",
    "            obs_prime = self.s_prime_buf[choice]\n",
    "        else:\n",
    "            obs = self.transform_observation(self.s_buf[choice]).T\n",
    "            obs_prime = self.transform_observation(self.s_prime_buf[choice]).T\n",
    "        actions = self.a_buf[choice]\n",
    "        rewards = self.r_buf[choice]\n",
    "        done = self.t_buf[choice]\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            for i in obs, actions, rewards, done, obs_prime:\n",
    "                print(i)\n",
    "        \n",
    "        q_pred = self.model.predict(obs)\n",
    "        if verbose:\n",
    "            print('q_pred before:')\n",
    "            print(q_pred)\n",
    "        q_pred_prime = self.target_model.predict(obs_prime)\n",
    "        target_pred = np.max(q_pred_prime, axis=1)\n",
    "        future_q = rewards + (1 - done) * self.discount * target_pred\n",
    "        if verbose:\n",
    "            print('future_q')\n",
    "            print(future_q)\n",
    "        \n",
    "        mask = np.arange(q_pred.shape[0]), np.int64(actions)\n",
    "        q_pred[mask] = (1 - self.lr) * q_pred[mask] + self.lr * future_q\n",
    "        if verbose:\n",
    "            print('q_pred after')\n",
    "            print(q_pred)\n",
    "        res = self.model.fit(obs, q_pred)\n",
    "    \n",
    "    def print_weights(self):\n",
    "        for l in self.model.get_weights():\n",
    "            print(l)\n",
    "    \n",
    "    def save_agent(self):\n",
    "        config = self.model.get_config()\n",
    "        weights = self.model.get_weights()\n",
    "        name = 'model' + datetime.datetime.now().isoformat(timespec='seconds').replace(':', ' ') + '.pickle'\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump((config, weights), f)\n",
    "            \n",
    "    def load_agent(self, name):\n",
    "        with open(name, 'rb') as f:\n",
    "            (config, weights) = pickle.load(f)\n",
    "\n",
    "        self.model = K.Sequential.from_config(config)\n",
    "        self.target_model = K.Sequential.from_config(config)\n",
    "        self.model.set_weights(weights)\n",
    "        self.save_target_model()\n",
    "        \n",
    "#for i in range(16):\n",
    "#    print(dq.pick_move(i, greedy=True))\n",
    "# for i in range(500):\n",
    "#     dq.save_sarts(np.random.choice(4, size=5))\n",
    "# dq.update_model()\n",
    "#dq.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c2b78-1982-4ff6-b2c5-6ada0427aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory_capacity = 100000\n",
    "agent = DeepQLearningAgent(eps_gen(1),\n",
    "                           env,\n",
    "                           replay_memory_capacity=replay_memory_capacity,\n",
    "                           lr=0.01,\n",
    "                           hidden=[4],\n",
    "                           size=2)\n",
    "rewards = []\n",
    "# fill initial replay memory buffer\n",
    "counter = 0\n",
    "while counter < replay_memory_capacity:\n",
    "    obs = env.reset()\n",
    "    while counter < replay_memory_capacity:\n",
    "        action = agent.pick_move(obs, random=True) # pick random moves for initial buffer\n",
    "        obs_prime, reward, done, info = env.step(action)\n",
    "        \n",
    "        agent.save_sarts([obs, action, reward, int(done), obs_prime])\n",
    "        obs = obs_prime\n",
    "        counter += 1\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd1672-cd07-47b4-96d0-931bbd8d1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0e953-d742-45e1-9eb5-7c8d6c7949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27239ac1-766f-48ba-bffb-a86faa941fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_eps_gen(eps_gen(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8beafa-1093-45fe-8191-dc67156d09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for episode in range(500):\n",
    "        obs = env.reset()\n",
    "        #for move in range(200):\n",
    "        while True:\n",
    "            action = agent.pick_move(obs, verbose=True)\n",
    "            obs_prime, reward, done, info = env.step(action)\n",
    "            agent.save_sarts([obs, action, reward, int(done), obs_prime])\n",
    "            obs = obs_prime\n",
    "            print_stats(env=None, agent=None, i=episode, clear=True)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        agent.update_model(size=1000, verbose=False)\n",
    "        rewards.append(reward)\n",
    "        if episode % 10 == 0:\n",
    "            agent.save_target_model()\n",
    "        \n",
    "    print('Done')\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755d50c-af9a-479e-9519-c48b53e2fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)#np.cumsum(rewards))# / np.arange(1, len(rewards)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29387e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    N = 100\n",
    "    for i in reversed(range(N)):\n",
    "        obs = env.reset()\n",
    "        obs = np.array([0, 0, i/N, 0])\n",
    "        env.env.state = obs\n",
    "        r = 0\n",
    "        while True:\n",
    "            # action = agent2.pick_move(obs, greedy=True,verbose=False)\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            print_stats(env=env, agent=None)\n",
    "            r += reward\n",
    "            if done:\n",
    "                break\n",
    "        print('Done', i/N, r)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416833d6-a765-4870-99cb-cb6417fe0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = DeepQLearningAgent(eps_gen(1), env)\n",
    "agent2.load_agent('model2022-04-22T13 27 18.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f4417-938f-40fb-99fb-f547367e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \\\n",
    "'''Done 0.2  147.0 Done 0.2 3.0\n",
    "Done 0.19 155.0 Done 0.19 24.0\n",
    "Done 0.18 173.0 Done 0.18 15.0\n",
    "Done 0.17 174.0 Done 0.17 25.0\n",
    "Done 0.16 177.0 Done 0.16 12.0\n",
    "Done 0.15 179.0 Done 0.15 28.0\n",
    "Done 0.14 215.0 Done 0.14 7.0\n",
    "Done 0.13 214.0 Done 0.13 29.0\n",
    "Done 0.12 225.0 Done 0.12 9.0\n",
    "Done 0.11 224.0 Done 0.11 15.0\n",
    "Done 0.1  299.0 Done 0.1 26.0\n",
    "Done 0.09 314.0 Done 0.09 14.0\n",
    "Done 0.08 316.0 Done 0.08 19.0\n",
    "Done 0.07 315.0 Done 0.07 27.0\n",
    "Done 0.06 321.0 Done 0.06 18.0\n",
    "Done 0.05 326.0 Done 0.05 35.0\n",
    "Done 0.04 336.0 Done 0.04 18.0\n",
    "Done 0.03 500.0 Done 0.03 19.0\n",
    "Done 0.02 500.0 Done 0.02 11.0\n",
    "Done 0.01 500.0 Done 0.01 14.0\n",
    "Done 0.0  500.0 Done 0.0 12.0'''\n",
    "for line in a.split('\\n'):\n",
    "    _, angle, reward, _, _, r2 = line.split()\n",
    "    print(angle + '\\t\\t\\t' + reward + '\\t\\t\\t' + r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2783d-91c4-4021-8c5c-02401a5e8065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3f900-a486-4c6e-bafc-c95f936986a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
